{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias Variance Trade-Off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lession we're going to discuss the \"Bias Variance Trade-Off\" and how you can use it to evaluate your **model's performance** and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bias Variance Trade Off is a fundamental topic of understanding your model's performance. \n",
    "\n",
    "> Review Chapter 2 of **Introduction to Statistical Learning** for a more in depth look.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The bias-variance trade-off is the point where we are just adding noise by adding model complexity or flexibility.\n",
    "- The **training error** goes down as it has to but the **test error** will start to go up.\n",
    "- The model after the bias trade-off begins to **overfit**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and discuss this topic by imagining a dartboard.\n",
    "\n",
    "![](https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg)\n",
    "\n",
    "- Imagine that the center of this target or a dartboard is a model that perfectly predicts the correct values.\n",
    "\n",
    "- As we move away from the bull's eye our predictions will get worse and worse. \n",
    "\n",
    "We're going to go ahead and make a quadrant of **low variance** versus **high variance** and **high bias** vs. **low bias**.\n",
    "\n",
    "So we can get an understanding of what the bias in variance terms mean generally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Imagine we can repeat our entire model building process to get a number of separate hits on the target.\n",
    "- Each hit represents an individual realisation of our model, given the chance variability in the training data we gather. \n",
    "\n",
    "![](https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sometimes we will get a good distribution of training data so we predict very well and we are close to the bull's eye.\n",
    "\n",
    "- \n",
    "While sometimes our training data might full of outliers or non-standard values resulting in poor predictions.\n",
    "\n",
    "![](https://www.kdnuggets.com/wp-content/uploads/bias-and-variance.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These different realizations result in a scatter of hits on the target or aiming for something for low bias and low variance.\n",
    "\n",
    "- But realistically which you'll have to do is tradeoff variance or bias.\n",
    "\n",
    "- And here we can see in the quarter of the target a **low variance low bias** model will predict correct values on the bull's eye.\n",
    "\n",
    "- A **low bias high variance** model will predicts values around the bullseye but with a high degree of variance.\n",
    "\n",
    "- Versus a **high bias low variance** model in that lower quadrant will have a high bias to a certain location but low variance. All your models predictions are in a certain area.\n",
    "\n",
    "- And in the worst **high variance high bias** means you just all over the place basically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A common temptation for beginners is to continually add complexity\n",
    "to a model until it fits the training set very well.\n",
    "\n",
    "Let's go ahead and begin to understand this in a machine learning algorithm, the linear regression.\n",
    "\n",
    "![](../imgs/bias-variance02.png)\n",
    "\n",
    "\n",
    "What you may want to do is let's say you are given a set of red training data. No your testing data just your training data.\n",
    "\n",
    "You might have a simple model such as the blue line and you get a certain error on your training data.\n",
    "\n",
    "And you decide as a beginner, hey maybe I should just make the model more and more complex or flexible so that it hits all those training\n",
    "points.\n",
    "\n",
    "However for hitting all those training points your model is going to fail to predict for new test points. Which is why we do that train test split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Doing that can cause a model to overfit to your training data and cause large errors on new data, such as the tests set. \n",
    "\n",
    "- Let's take a look at an example model on how we can see overfitting occur from a error standpoint using test data.\n",
    "\n",
    "- We'll use a black curve with some noise points off of it to represent the true shape taht the data follows.\n",
    "\n",
    "![](../imgs/bias-variance03.png)\n",
    "\n",
    "We have three images here:\n",
    "\n",
    "### Left plot\n",
    "\n",
    "The first one is the $X$ versus the $Y$.\n",
    "\n",
    "And here we have **model flexibility** as different linear fits:\n",
    "\n",
    "- A **linear fit** \n",
    "- A **quadratic fit** \n",
    "- Or a **spline fit** \n",
    "\n",
    "with each one getting more complex.\n",
    "\n",
    "So the simplest is just a linear fit more complicated that is quadratic and, then you can have a spline fit. \n",
    "\n",
    "And you'll notice that the black curve is the truth that the model\n",
    "actually follows. So all the points are just noise around the actual black curve which is the truth.\n",
    "\n",
    "In order to evaluate your models and compare the complexities to each other, which you'll have to do is plot out the complexity or flexibility of the model.\n",
    "\n",
    "### Middle lot\n",
    "\n",
    "For instance the polynomial level of a regression fit versus the error metric such as **mean square error**. You can see here in that middle plot that's exactly what we've done. \n",
    "\n",
    "We've plotted out for the **training data** versus the **test data**. You'll notice all the way to the left, we have a simple model, that yellow linear model. For this model we have a high error on both the test data and the training data.\n",
    "\n",
    "As we begin to get more complicated with the quadratic model, in blue, we begin to lower the error for the test data and the training data.\n",
    "\n",
    "As we get even more complex however, we begin to lower the error for the training data significantly but, as a result, you begin to have raised the error on the test data, the new data. \n",
    "\n",
    "So if you want to find the point that's going to balance out the bias and the variance. In this particular case it's going to be closer to the quadratic fit that blu point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "So you want to balance out the bias and variance of your model to the point where your test data and training data have reached some sort of minimum and grouping together.\n",
    "\n",
    "This is the classic plot to show this as a general stance, where you have low versus high model complexitybon the X-axis and some sort of prediction error on the y axis.\n",
    "\n",
    "\n",
    "![](../imgs/bias-variance04.png)\n",
    "\n",
    "And as you moved to the left you get a higher bias but lower variance.\n",
    "\n",
    "And as he moves to the right to a higher complexity model you get a lower bias the high variance.\n",
    "\n",
    "what you want to do is pick a point where you are comfortable with the bias tradeoff.\n",
    "\n",
    "If you go to the left of it you'll start to under fit to the data and if you go to the right of it you'll start to overfit the data.\n",
    "\n",
    "Meaning you're hitting all of those points and your training data and new data is cutting a larger error because of that.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep that in mind as we continue on using Python for machine learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
