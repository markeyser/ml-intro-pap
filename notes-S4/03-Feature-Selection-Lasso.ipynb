{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection - LASSO regularization\n",
    "\n",
    "## Introducition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will cover regularisation as a method for feature selection for linear models.\n",
    "\n",
    "\n",
    "Regularisation consists in adding a penalty on the different parameters of the model to reduce the freedom of the model. Hence, the model will be **less likely to fit to the noise** of the training data and will improve the generalization abilities of the machine learning algorithm.\n",
    "For linear models, there are in general 3 types of regularization: \n",
    "\n",
    "- The L1 regularization (also called Lasso)\n",
    "- The L2 regularization, (also called Ridge)\n",
    "- The L1/L2 regularization (also called Elastic Net)\n",
    "\n",
    "I will focus on the L1 and L2 for comparison, to conclude that LASSO is the regularization that allows for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization: LASSO\n",
    "\n",
    "As I mentioned regularization consists in applying a penalty to the coefficients that multiply each of the predictors in the linear model,\n",
    "in order to avoid overfitting. \n",
    "\n",
    "\n",
    "$$\\frac{1}{2m}x\\sum(y - ypred)^2 + \\lambda\\sum \\beta^1$$\n",
    "\n",
    "- $m$ = number of observations\n",
    "- $y$ = observed output\n",
    "- $ypred$ = predicted output\n",
    "- $\\lambda$ is the regularization parameter\n",
    "\n",
    "The higher the penalty, typically the bigger the generalization. If the penalty is too high, however, the model may lose predictive power.\n",
    "\n",
    "During fitting of the algorithm, what the machine learning model is trying to minimize, is the difference between the predicted outcome and the real value of the observation, plus the regularization component. \n",
    "\n",
    "The regularization component is a penalty on the coefficients that the linear model feeds to the variables.\n",
    "\n",
    "You can see that to keep this equation to the minimum, if we increase lambda, that is if we increase the penalty, we need to decrease $\\beta$ which are the coefficients.\n",
    "\n",
    "> L1 / Lasso will **shrink some parameters to zero**, therefore allowing for feature elimination\n",
    "\n",
    "The LASSO regularization has the property that it can shrink some of the parameters, I mean some of the coefficients, to zero. This means that some of the $\\beta$ can be zero.\n",
    "\n",
    "Therefore, the regularisation indicates that a certain predictor or a certain variable will be then multiplied by zero to estimate the target. And therefore, it will not add to the overall final prediction of the output. This means, in other words, that that feature can be removed as it does not contribute to the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization: RIDGE\n",
    "\n",
    "The ridge regression or L2, is different from LASSO in that what is trying to minimize is the Theta square.\n",
    "\n",
    "\n",
    "$$\\frac{1}{2m}x\\sum(y - ypred)^2 + \\lambda\\sum \\beta^2$$\n",
    "\n",
    "- $m$ = number of observations\n",
    "- $y$ = observed output\n",
    "- $ypred$ = predicted output\n",
    "- $\\lambda$ is the regularization parameter\n",
    "\n",
    "> L2 / Ridge, as the penalization increases, the coefficients approach zero but do not equal zero, hence no variable is ever excluded.\n",
    "\n",
    "\n",
    "As the penalization lambda increases, the coefficients of the regression approach zero\n",
    "but it never equals zero. Therefore, this regularization is not suitable for feature selection.\n",
    "\n",
    "It is for a model optimization but it will not allow you to select variables, or better say, remove variables from a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see an example for more clarity. In this example, we want to predict the prices of houses from the house sale dataset on Kaggle. \n",
    "\n",
    "### LASSOO\n",
    "\n",
    "![](../imgs/L1.png)\n",
    "\n",
    "These plots show the value of the coefficients that multiply each of the predictors on the right\n",
    "as we increase the regularization parameter lambda. These are the predictors, these are the different lambdas or penalties, clearly and as expected, as we increase lambda we penalize\n",
    "the parameters harder, therefore the value of the coefficients decrease.\n",
    "\n",
    "For the LASSO regularization, we can see that as we increase Lambda, one after the other, the different feature coefficients are drawn to zero.\n",
    "\n",
    "So as the regularization increases, the features are eliminated one after the other until we eliminate all of the features if the regularization is too high.\n",
    "\n",
    "### RIDGE\n",
    "\n",
    "![](../imgs/L2.png)\n",
    "\n",
    "For the ridge regression, likewise, when the penalty increases, the coefficients that are fit to the variables also decrease but they decrease altogether and they do not reach zero. Therefore, these regularization is not suitable for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example\n",
    "\n",
    "Here we have another example on a different dataset.\n",
    "\n",
    "![](../imgs/L1L2.png)\n",
    "\n",
    "Again, here are the different features and in this opportunity the plot is so that the increase in lambda goes towards the left, these are bigger lambdas, and these are smaller lambdas. In this case, the penalty then increases towards the left. We can see again how the coefficients of the different variables are shrunk to zero, one after the other, with the LASSO regularization. Therefore allowing for feature selection. On the contrary, the ridge regularization shrinks all the coefficients at the same time, only reaches zero at very high penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EMBEDDED METHODS: LASSO\n",
    "\n",
    "Therefore, as you can see, by fitting a linear or logistic regression with the LASSO regularization,\n",
    "we can then evaluate the coefficients of the different variables and remove those which coefficients\n",
    "are zero.\n",
    "\n",
    "And in this way, we are selecting features, while fitting the machine learning algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
