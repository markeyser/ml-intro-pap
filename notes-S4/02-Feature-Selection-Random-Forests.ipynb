{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, I will cover feature selection by tree based machine learning algorithms\n",
    "derived feature importance. \n",
    "\n",
    "### Decission trees \n",
    "\n",
    "- Most popular machine learning algorithms\n",
    "- Highly accurate\n",
    "- Good generalization (low overfitting)\n",
    "- Interpretability\n",
    "\n",
    "Random forests and decision trees in general are very popular machine learning algorithms.\n",
    "\n",
    "They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. \n",
    "\n",
    "This interpretability is given by the fact that it is straight forward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable contributes to the decision. \n",
    "\n",
    "![](../imgs/decisiontree.png)\n",
    "\n",
    "An ensemble of decision trees like random forests are a collection of single decision trees.\n",
    "\n",
    "Each tree is a series of questions if you want, based on different features of the data set.\n",
    "\n",
    "For each feature, a question is formulated so that the answer to your question leads to the highest possible decrease of impurity, or in other words, to the best possible separation of classes into groups that contain only one class or at least the majority of one class. \n",
    "\n",
    "At each node, this is at each question, the tree divides the dataset into two buckets each of them hosting observations that are more similar among themselves and different from the observations in the other bucket. Therefore the importance of each feature is derived by how pure each of the buckets is. \n",
    "\n",
    "For classification, the measure of impurity is either to Gine any or the information gain or Entropy. \n",
    "\n",
    "For regression, the measure of impurity is the variance. \n",
    "\n",
    "Therefore when training a tree, it is possible to compute how much each feature decreases the impurity or in other words, how good the feature is at separating the classes. The more a feature decreases the impurity the more important the feature is.\n",
    "\n",
    "Features selectors at higher nodes lead typically to the greater gains in purity and are therefore the most important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST IMPORTANCE\n",
    "\n",
    "- Random Forests consist of several hundreds of individuals decission trees\n",
    "- The impurity decreases for each feature is averaged across tress\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "- Correlated features show equal or similar importance\n",
    "- Corrleated features importance is lower than the real importance, determined when tree is built in absence of correlated counterparts\n",
    "- Highly cardinal variables show greater importance (trees are biased to this type of variables)\n",
    "\n",
    "\n",
    "Random forest consist of 4 to 12 hundred of decision trees. Each of them built over a random extraction of the observations from the dataset and a random extraction\n",
    "of the features.\n",
    "\n",
    "Not every tree sees all the features or all the observations and this guarantees that the trees are\n",
    "decorrelated and therefore less prone to overfitting.\n",
    "\n",
    "\n",
    "Each tree is also a sequence of yes-no questions based on a single combination of features. In random forest, the impurity decrease from each feature can be average across all the trees to determine the final importance of the variable.\n",
    "\n",
    "A few things to keep in mind though, when examining feature importance by using tree based methods.\n",
    "\n",
    "First, is that correlated features show equal or similar importance but that importance is smaller than if the feature was present in the dataset without its correlated counterpart.\n",
    "\n",
    "\n",
    "Also trees tend to overfit to highly cardinal variables. This is categorical variables with a lot of categories. Therefore they will show these variables as highly important but they are actually interfering with the generalization of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how can we go about select features using tree derived importance? \n",
    "\n",
    "- Build a random forest\n",
    "- Determine feature importance\n",
    "- Select the features with highest importance\n",
    "- There is a scikit-learn implementation for this\n",
    "\n",
    "Simple. We first build a random forest. Then look at the importance derived from the random forest for each feature and, then we select the features with the highest importance.\n",
    "\n",
    "In fact, there is a sklearn implementation to do this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive feature elimination\n",
    "\n",
    "- Build random forest\n",
    "- Calculate feature importance\n",
    "- Remove least importance feature\n",
    "- Repeat till a condition is met\n",
    "\n",
    "> If the feature removed is correlated to another feature in the dataset, by removing the correlated feature, the true importance of the other feature will be revealed -> its importance wil increase.\n",
    "\n",
    "As an alternative, we can do recursive feature elimination. This procedure consists in building a random forest, derive the importance for each feature, remove the feature with the least importance, and then build a random forest again with the remaining features, derive the importance for each feature again and remove again the feature with the least importance and repeat until certain condition is met.\n",
    "\n",
    "This condition can be for example a certain number of selected features. \n",
    "\n",
    "In theory, this method is better if there are a high number of correlated features to select from.\n",
    "And this is because as I mentioned before increased correlated features will show similar importance\n",
    "though smaller. So if you remove the correlated feature the remaining features importance will increase. \n",
    "\n",
    "If we didn't do recursive feature elimination we may remove the two features that are correlated because they show smaller importance, but if we do a recursive feature elimination by removing one when the importance of the remaining one increases, we will be able to keep it. \n",
    "\n",
    "In practice, I have not seen a real boost by using this implementation and it is actually very computationally expensive, but give it a go and see what happens with your datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADIENT BOOSTED TREES FEATURE IMPORTANCE\n",
    "\n",
    "- Feature importance calculated in the same way\n",
    "- Biased to highly cardinal features\n",
    "- Importance is susceptible to correlated features\n",
    "- Interpretability of feature importance is not so strainghtforward:\n",
    "\n",
    "    - Later trees fit to the errors of the first trees, therefore feature importance is not necessarily proportional on the influence of the feature on the outcome, rather on the mistakes of the previous trees.\n",
    "    - Averaging across trees may not add much information on true relation between feature and target\n",
    "\n",
    "\n",
    "For gradient boosted trees, feature importance is calculated in the same way as for random forest.\n",
    "\n",
    "This is better features are those that lead to the best separation of classes or higher decreasing entropy.\n",
    "\n",
    "They are, as well biased to highly cardinal categorical variables and also susceptible to co-related features.\n",
    "\n",
    "The interpretability of the feature importance is not as straightforward as for random forest and this is because in gradient boosted trees subsequent trees try to minimize the error made by previous trees.\n",
    "\n",
    "Therefore feature importance is not necessarily proportional to the influence of that feature on the target but rather integrated with the importance on correcting the mistakes from previous trees.\n",
    "Averaging across trees then may not add a lot of information on the true relationship between the\n",
    "feature and the target.\n",
    "\n",
    "However it is a valid method to select features and you can use it in the same way as you do to select features from random forests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
