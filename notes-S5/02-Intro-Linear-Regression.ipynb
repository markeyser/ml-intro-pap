{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Assignment\n",
    "\n",
    "> Chpaters 2 & 3 of **Introduction to Statistical Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "Let's go ahead and begin to plot out the sort of example let's cut to the regression with only two data points which is the simplest possible example.\n",
    "\n",
    "![](../imgs/linear01.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we're trying to do when we calculate our regression line is draw a line\n",
    "that's as close to every dot as possible. \n",
    "\n",
    "![](../imgs/linear02.png)\n",
    "\n",
    "For **classic linear regression** or the **least squares** method, you only measure the closeness in the up and down direction.\n",
    "\n",
    "Here we have a perfectly fitted line because we only had two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now wouldn't it be great if we could apply this same concept to grasp with more than just two data points.\n",
    "\n",
    "![](../imgs/linear03.png)\n",
    "\n",
    "By doing this we could take multiple men and their sons heights and do things like tell me how tall we expect the son to be before he even has a son. And this is the idea behind supervised learning.\n",
    "\n",
    "We're going to have a bunch of labeled data points create a model, in this case a regression and try to take unlabeled data such as a father's high and spit back out labeled data our prediction of the sun's height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal if linear regression is to **minimize the vertical distance** between all the data points in our line.\n",
    "\n",
    "![](../imgs/linear04.png)\n",
    "\n",
    "So in determining the **best line** we are attempting to minimize the distance between all the points and distance to our line.\n",
    "\n",
    "There are lots of actually different ways to minimize this, the **sum of squares errors**, the **sum of absolute errors**, etc..but all of these methods have a general goal of minimizing the distance between your line and the rest of the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example one of the most popular methods that we just described is the least squares method.\n",
    "\n",
    "![](../imgs/linear05.png)\n",
    "\n",
    "Here we have a couple of blue data points along an x and y axes and we want to fit a linear regression line. And the question is how do we decide which line is the best fitting one?\n",
    "\n",
    "![](../imgs/linear06.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can go ahead and use the **Least Squares** method which we discussed earlier.\n",
    "\n",
    "This method is fitted by minimizing the **sum of the squares of the residuals**.\n",
    "\n",
    "The residuals for an observation is the difference between the observation the `y` value and the fitted line.\n",
    "\n",
    "![](../imgs/linear07.png)\n",
    "\n",
    "In this image the residuals are marked by the red line. The difference between the true data point in blue and your fitted model line, the black diagonal line."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
